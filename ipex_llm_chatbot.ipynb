{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6f0702d-3d3a-496e-a138-e5c5e0e9701c",
      "metadata": {
        "id": "c6f0702d-3d3a-496e-a138-e5c5e0e9701c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "from ipex_llm.transformers import AutoModelForCausalLM\n",
        "from transformers import AutoTokenizer, TextIteratorStreamer\n",
        "from time import perf_counter\n",
        "from threading import Thread\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "962170a2-1237-412c-951e-7a3ab217c369",
      "metadata": {
        "id": "962170a2-1237-412c-951e-7a3ab217c369"
      },
      "outputs": [],
      "source": [
        "#load model from HF and optimize\n",
        "model_id = \"NousResearch/Meta-Llama-3-8B-Instruct\"\n",
        "model_FP32 = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "model_INT4 = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                                 trust_remote_code=True,\n",
        "                                                 load_in_4bit=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "#define token termination\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dd66993-c4fd-4640-a444-98b862b114bb",
      "metadata": {
        "id": "0dd66993-c4fd-4640-a444-98b862b114bb"
      },
      "outputs": [],
      "source": [
        "#define the chatbot function to be used in Gradio UI\n",
        "def chatbot(model_precision,Question):\n",
        "\n",
        "    #dropdown selection - redirect to default or quantized model\n",
        "    if model_precision == 'FP32':\n",
        "        model = model_FP32\n",
        "    else:\n",
        "        model = model_INT4\n",
        "\n",
        "    #input message and tokenize\n",
        "    messages = [{\"role\":'user', 'content': Question}]\n",
        "    input_ids = tokenizer.apply_chat_template(messages,add_generation_prompt=True,return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    st = time.time()\n",
        "\n",
        "    generate_kwargs = dict(\n",
        "        {\"input_ids\": input_ids},\n",
        "        max_new_tokens=512,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        streamer=streamer,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "    #run model in threading\n",
        "    new_token = \"\"\n",
        "    t = Thread(target= model.generate, kwargs=generate_kwargs)\n",
        "    t.start()\n",
        "    chatbot = []\n",
        "    response_message = \"\"\n",
        "\n",
        "    #create live text streamer\n",
        "    streamer = TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens = True)\n",
        "\n",
        "    for new_token in streamer:\n",
        "        #if new_token != '<':\n",
        "            response_message += new_token\n",
        "            tokenz = tokenizer.tokenize(response_message)\n",
        "            num_tokens = len(tokenz)\n",
        "            et = time.time() - st\n",
        "            yield response_message, f'token:{num_tokens}   time:{round(et,2)}   token/sec: {round(num_tokens/et,2)}' #round(num_tokens)#/et,2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#define the gradio UI interface\n",
        "demo = gr.Interface(\n",
        "    max_batch_size=1,\n",
        "    delete_cache=(5, 5),\n",
        "    fn=chatbot,\n",
        "    inputs=[gr.Dropdown([\"FP32\", \"INT4\"],value = \"INT4\", label=\"Select Precision -if none selected, Default will be INT4\"),\n",
        "            gr.Textbox(label=\"Ask Me Anything\",lines=5)],\n",
        "    outputs=[gr.Textbox(label=\"Answers\"),\n",
        "             gr.Textbox(label=\"Tokens/sec\")],\n",
        "    allow_flagging=False,\n",
        "    title=\"Intel Pytorch Extension (IPEX-LLM) Chatbot\",\n",
        "    description=\"\"\"<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/6/64/Intel-logo-2022.png\" width=200px>\n",
        "    <h2>Inferenced on <u>Azure Compute D32ds v6</u> with AMX Acceleration</h2></n>\n",
        "    Using NousResearch/Meta-Llama-3-8B-Instruct</center>\"\"\",\n",
        "    article=\"\"\"<h3>Built by Malcolm Chan</h3></n>\n",
        "    If you have stopped the answer generation abruptly, please press <strong>Clear</strong> to purge the history cache before re-using\"\"\",\n",
        "    theme=gr.Theme.from_hub('HaleyCH/HaleyCH_Theme')\n",
        ")\n",
        "\n",
        "#launch gradio demo\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "id": "dK2T3RpEZsTp"
      },
      "id": "dK2T3RpEZsTp",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}